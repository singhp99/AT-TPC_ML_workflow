{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AT-TPC Machine Leaning Workflow","text":"<p>Hello! I am Pranjal Singh and welcome to the documentation site for the AT-TPC ML workflow. All work here is in progress and is subject to change. </p> <p>I am a fourth year PhD candidate at Michigan State University and this project is a part of my thesis work, but certainly not possible without the help of all members of the AT-TPC group and our collaborators. </p>"},{"location":"contributors/","title":"Contributors","text":"<p>This work would not be possible without the dedication of other AT-TPC group members and our collaborators. Additionally, I would also like to acknowledge all the contributors to the experimental data acquisition at the Facility for Rare Isotope Beams (FRIB)\u2014where this experiment took place.</p> <p>Below is a list of all AT-TPC members that contributed to this project, in no particular order.</p> <ul> <li>Pranjal Singh - singhp99</li> <li>Daniel Bazin - DBazin</li> <li>Gordon McCann - gwm17</li> <li>Zach Serikow - sigmanotation</li> <li>Nathan Turi - turinath</li> <li>Till Schaeffeler - Till-Schaeffeler</li> <li>Daniela Ramirez - danielaramirez415</li> </ul> <p>Special thanks to the ALPhA group at Davidson College for their help with the ML resources and PointNet model base.</p> <p>Machine learning, and specifically training a model, requires computation resources that are not available on our personal devices. This project would not be possible without the High Performance Computational Cluster (HPCC) resources made available by the Michigan State University's ICER group.</p> <p>This repository is maintained by Pranjal Singh and any questions about it can be directed to singhp19@msu.edu.</p>"},{"location":"about/about_det/","title":"AT-TPC","text":"<p>In order to be able to inspect the AT-TPC data, it may be helpful to first take a look at what the AT-TPC looks like physically; this will help us understand why certain quantities would be conserved. </p> Figure 1: A schematic view of the AT-TPC detector <p>From the Fig. 1, we can see that this detector is cylindrical, where the beam axis is typically set as the z axis. The \"active\" part of the detector corresponds to a gas being used as the target, therefore this is a gaseous detector. Due to this reason, we use inverse kinematics\u2014where the heavy particle is the beam and the lighter in a gas form inside the detector. </p> <p>For the purposes of this work, I will only be looking at data that is derived from a particular experiment. </p> \\[ ^{16}\\text{O} + \\alpha \\] <p>An ionized beam, where electrons are stripped off, of \\(^{16}\\text{O}\\) comes in through the beam entrance and enters the active volume of the detector, the beam interacts with the gas and creates charged particles that in turn ionize the gas\u2014creating electrons. These electrons drift to the end of the detector, moved by the electric field being point that way and the magnetic field (a solenoid magnet around the detector) curving the path of the particles. The electrons get sensed by the pad plane, creating a 2-D image; we can get another dimension through the beam axis by converting the drift velocity of the electrons into position\u2014finally getting 3-D images of the reactions. Depending on the event (reaction), there are any number of tracks (charged particles) and our data can be seen in the form of point clouds, which are higher dimensional tensors. </p> <p>If you would like to know more about why we decided to stray into the forest of ML, you can read about it on this page.</p>"},{"location":"about/about_pn/","title":"PointNet","text":"<p>As mentioned in the \"Why ML?\" section, though our data looks like an image, it is not exactly set up in a stacked matrix, but rather with each point having the same four properties (x, y, z, charge); this format is referred to as pointclouds. </p> <p>This disables us from using a convolutional neural network, as those work best with stacked matrices where different filters (other matrices) are applied for transformation. However, there are specific model architectures, that are neural nets, built for the point cloud format specifically; we have chosen to use one called PointNet. This model was coined by Qi et al. at Stanford University in 2017 (see here for the publication). </p> Figure 6: PointNet architecture <p>Fig. 6 shows the different purposes for the PointNet model, the part segmentation and semantic segmentation, though useful, is too advanced for our purposes\u2014we only require the classification tool. It is important to note that the pointclouds used by Qi et al. are objects, not akin to the tracks that are seen in the AT-TPC data; we can already foresee a possible source of trouble. </p> <p>If you are interested in getting started with this model, there is a GitHub repository managed by the authors of model here.</p> <p>If you would like to learn more about the model architecture (and how PointNet works), below are some resources from YouTube that provide an in depth explanation.  </p> <ul> <li> <p>Talk by Charles R. Qi (author) at Conference on Computer Vision and Pattern Recognition (CVR) 2017</p> </li> <li> <p>Paper Explained by Aldi Piroli </p> </li> <li> <p>Lecture by Maziar Raissi</p> </li> </ul> <p>The model version I am using was adapted from a reference by Emilio Villasana, Andrew Rice, Raghu Ramanujan, and Dylan Sparks from the ALPhA group at Davidson College. </p>"},{"location":"about/why_ml/","title":"Why ML?","text":"<p>One may puzzle at the connection between our data and machine learning, and they would be fair to do so. It will be useful, perhaps, to see what the data looks like.</p> Figure 2: Point clouds for different categories of events <p>Applied ML research has many avenues of implementation and a substantial pick of tools; convolutional neural networks (CNN) have had some recent development in image based research, and fortunately for us, our data closely mimics an image. Fig. 2 shows the different categories of events corresponding to different types of reactions, and they could be considered classes for a classification model, which is the route we have chosen to take. </p> <p>Though our data looks like an image, normally used for CNN, it is not exactly an image. Fig. 3 shows how a CNN would work with an image, but it also shows that the data must be in a form of binary arrays. </p> Figure 3: Convolutional Neural Network   [Image Source] <p>This not what our data looks like, in fact, our data is more like a high dimensional tensor\u2014shown in Fig. 4\u2014where each point in a singular point cloud that has x,y,z position coordinates and charge. We have multiple runs, and each run has many events, where each event has different amounts of points that have the same four properties; well we have more, but these are better suited for the purposes of training our model. </p> Figure 4: AT-TPC data tensor <p>Fortunately, there have been recent developments in machine learning architecture specifically for point cloud data, we will be using one of these neural network called PointNet. You can learn more about this architecture here.</p> <p>However, this doesn't explain our need for using ML techniques or how we would apply it in our analysis process. Our current analysis involves using a complex algorithm called Spyral. This involves many phases and steps, but the step of relevance in our context includes a clustering algorithm called HDBSCAN (more information here). This clustering, though extremely helpful for most of our data (1 or 2 track events), isn't the best at clustering the rare multi-track events (3 or higher tracks). The particular experiment I am studying necessitates finding five track events\u2014corresponding to a specific type of decay involving five \\(/alpha\\) particles. </p> Figure 5: Kinematics plot of AT-TPC data, with a red box indicating problematic points <p>Fig. 5 shows evidence of where we know our current analysis fails, the red box indicated point we should not see but still do. Furthermore, the events I am interested in lie between 10-60 degrees and are very rare, other events with one or two tracks\u2014greatly abundant in our data\u2014hide the events of insert. If I was able to sort only the five track events prior to the analysis, I would not only be able to remove the events not of importance but also increase the efficiency of our analysis (since I would be analyzing less data).</p>"},{"location":"api/attpc_noise/","title":"ATTPC Noise Addition","text":""},{"location":"api/attpc_noise/#at-tpc-noise-addition","title":"AT-TPC Noise Addition","text":""},{"location":"api/attpc_noise/#only-for-use-with-simulated-data","title":"Only for use with simulated data","text":"<p>This method is used only when training with simulated data.</p> <p>It is very difficult to exactly replicate the AT-TPC noise, but we can closely mimic it by creating noise that is uniformly random in z with <code>np.random.randint(0,1000)</code> and Gaussian in r with <code>r_noise = np.random.normal(0, 50, (data_size,1))</code>.</p> <p>Can be used to add AT-TPC like noise by using <code>np.random.randint()</code> between <code>[-50,50]</code> for x and y, and <code>[0,1000]</code> for z. </p> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Adding noise in cylindrical coordinates to simulate AT-TPC noise</p>"},{"location":"api/attpc_noise/#scripts.ml_preprocessing_steps.AttpcNoiseAddition--parameters","title":"Parameters","text":"<p>ratio_noise : (float)     Fraction of noise to length of point cloud to be produced</p>"},{"location":"api/attpc_noise/#scripts.ml_preprocessing_steps.AttpcNoiseAddition--returns","title":"Returns","text":"<p>new_data: (array)     Data with AT-TPC noise added event_lengths: (array)     New event lengths accounting for added noise points</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>class AttpcNoiseAddition(BaseEstimator,TransformerMixin):\n    \"\"\"Adding noise in cylindrical coordinates to simulate AT-TPC noise\n\n    Parameters\n    ----------\n    ratio_noise : (float)\n        Fraction of noise to length of point cloud to be produced\n\n    Returns\n    ----------\n    new_data: (array)\n        Data with AT-TPC noise added\n    event_lengths: (array)\n        New event lengths accounting for added noise points \n    \"\"\"\n    def __init__(self, ratio_noise: float):\n        self.ratio_noise = ratio_noise\n\n\n    def fit(self,X,y=None):\n        return self  \n\n    def transform(self,X,y=None):\n        \"\"\"Adding AT-TPC like noise with cylindrical dataset\n\n        Args:\n            X (tuple):Packed data and event lengths np.array\n            y (None): Defaults to None.\n\n        Returns:\n            (tuple): Data with noise and new event lengths\n        \"\"\"\n        data,event_lengths = X\n        skipped = 0\n        for i in tqdm.tqdm(range(len(data)), desc=\"Adding Noise\"): \n            data_size = int((self.ratio_noise)*event_lengths[i])\n\n            noise_z = np.random.randint(0,1000,(data_size,1))\n            r_noise = np.random.normal(0, 50, (data_size,1))\n\n            theta_noise = np.random.uniform(0, 2*np.pi, (data_size,1))\n\n            noise_x = r_noise * np.cos(theta_noise)\n            noise_y = r_noise * np.sin(theta_noise)\n\n            array_charge = np.zeros((data_size,1))\n            noise_data = np.concatenate((noise_x,noise_y,noise_z,array_charge),axis=1)\n            combined_data = np.concatenate((data[i,:event_lengths[i],:],noise_data,data[i,-2:,:]),axis=0)\n\n            if combined_data.shape[0] &gt; data.shape[1]:\n                skipped+=1\n                continue\n\n            data[i, :combined_data.shape[0], :] = combined_data\n\n            event_lengths[i]+=data_size\n\n\n        print(f\"Number of events skipped: {skipped}\")\n        return (data,event_lengths)\n</code></pre>"},{"location":"api/attpc_noise/#scripts.ml_preprocessing_steps.AttpcNoiseAddition.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Adding AT-TPC like noise with cylindrical dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>tuple</code> <p>Packed data and event lengths np.array</p> required <code>y</code> <code>None</code> <p>Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Data with noise and new event lengths</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>def transform(self,X,y=None):\n    \"\"\"Adding AT-TPC like noise with cylindrical dataset\n\n    Args:\n        X (tuple):Packed data and event lengths np.array\n        y (None): Defaults to None.\n\n    Returns:\n        (tuple): Data with noise and new event lengths\n    \"\"\"\n    data,event_lengths = X\n    skipped = 0\n    for i in tqdm.tqdm(range(len(data)), desc=\"Adding Noise\"): \n        data_size = int((self.ratio_noise)*event_lengths[i])\n\n        noise_z = np.random.randint(0,1000,(data_size,1))\n        r_noise = np.random.normal(0, 50, (data_size,1))\n\n        theta_noise = np.random.uniform(0, 2*np.pi, (data_size,1))\n\n        noise_x = r_noise * np.cos(theta_noise)\n        noise_y = r_noise * np.sin(theta_noise)\n\n        array_charge = np.zeros((data_size,1))\n        noise_data = np.concatenate((noise_x,noise_y,noise_z,array_charge),axis=1)\n        combined_data = np.concatenate((data[i,:event_lengths[i],:],noise_data,data[i,-2:,:]),axis=0)\n\n        if combined_data.shape[0] &gt; data.shape[1]:\n            skipped+=1\n            continue\n\n        data[i, :combined_data.shape[0], :] = combined_data\n\n        event_lengths[i]+=data_size\n\n\n    print(f\"Number of events skipped: {skipped}\")\n    return (data,event_lengths)\n</code></pre>"},{"location":"api/data_augmentation/","title":"Data Augmentation","text":""},{"location":"api/data_augmentation/#data-augmentation","title":"Data Augmentation","text":"<p>We have azimuthal symmetry within our data, that is, if we rotate the data about the z (beam) axis, the result conserves the physics. We can then \"create\" copies of any event by rotating it around beam axis by random angles between <code>[0,2\u03c0]</code>.</p> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p>"},{"location":"api/data_augmentation/#scripts.ml_preprocessing_steps.DataAugumentation--parameters","title":"Parameters","text":"<p>target_size: (int)      Which is the number of point of the second dimension</p>"},{"location":"api/data_augmentation/#scripts.ml_preprocessing_steps.DataAugumentation--return","title":"Return","text":"<p>augmented_data: (np.array)     Increased shape of array by the number of augmented events for class 3 and 4</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>class DataAugumentation(BaseEstimator,TransformerMixin):\n    \"\"\"\n    Parameters\n    ----------\n    target_size: (int) \n        Which is the number of point of the second dimension\n\n    Return\n    ----------\n    augmented_data: (np.array)\n        Increased shape of array by the number of augmented events for class 3 and 4\n\n    \"\"\"\n    def __init__(self,target_size):\n        self.target_size = target_size\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X,y=None):\n        \"\"\"Adding more data by creating copies with azimuthal symmetry around z-axis\n\n        Args:\n            X (np.array): data array \n            y (None): Defaults to None\n\n        Returns:\n            (np.array): augmented data array\n        \"\"\"\n        labels = X[:,-2,0].astype(int)\n        class_dist = np.array([np.sum(labels==i) for i in range(5)]) #there are 5 labels (0-5) \n        print(f\"Data shape before data augmentation: {X.shape}\")\n        print(f\"The class distribution before augmentation: {class_dist}\")\n\n        multipliers = {0: 2, 1: 2, 2: 2, 3: 2, 4: 2}\n        augmented_length = sum(class_dist[c] * m for c, m in multipliers.items()) #this will account for multiplier increase (ai helped here)\n        augumented_data = np.full((augmented_length+len(X),X.shape[1],X.shape[2]),np.nan)\n        augumented_data[:len(X)] = X #filling up the original events \n        new_start = len(X)\n        current_idx = len(X)\n        for i in range(len(X)):\n            label = labels[i] \n            multiplier = multipliers[label]\n\n            event = X[i]\n            event_points = event[:-2]\n\n            for j in range(multiplier):\n                theta = np.random.uniform(0, 2 * np.pi) #rotation about the z-axis\n                cos, sin= np.cos(theta), np.sin(theta) #need to get the conversion\n                points_rot = event_points.copy() #don't want to change the original points \n                x, y = points_rot[:,0],points_rot[:,1] #original x and y points \n                points_rot[:,0] = cos * x - sin * y \n                points_rot[:,1] = sin * x + cos * y \n\n                augumented_data[current_idx] = np.concatenate([points_rot, event[-2:]], axis=0)\n                current_idx+=1\n        labels = augumented_data[:,-2,0].astype(int)\n        class_dist = np.array([np.sum(labels==i) for i in range(5)]) #there are 5 labels (0-5) \n        print(f\"The class distribution after augumentation: {class_dist}\")\n\n\n        return augumented_data\n</code></pre>"},{"location":"api/data_augmentation/#scripts.ml_preprocessing_steps.DataAugumentation.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Adding more data by creating copies with azimuthal symmetry around z-axis</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>data array </p> required <code>y</code> <code>None</code> <p>Defaults to None</p> <code>None</code> <p>Returns:</p> Type Description <code>array</code> <p>augmented data array</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>def transform(self,X,y=None):\n    \"\"\"Adding more data by creating copies with azimuthal symmetry around z-axis\n\n    Args:\n        X (np.array): data array \n        y (None): Defaults to None\n\n    Returns:\n        (np.array): augmented data array\n    \"\"\"\n    labels = X[:,-2,0].astype(int)\n    class_dist = np.array([np.sum(labels==i) for i in range(5)]) #there are 5 labels (0-5) \n    print(f\"Data shape before data augmentation: {X.shape}\")\n    print(f\"The class distribution before augmentation: {class_dist}\")\n\n    multipliers = {0: 2, 1: 2, 2: 2, 3: 2, 4: 2}\n    augmented_length = sum(class_dist[c] * m for c, m in multipliers.items()) #this will account for multiplier increase (ai helped here)\n    augumented_data = np.full((augmented_length+len(X),X.shape[1],X.shape[2]),np.nan)\n    augumented_data[:len(X)] = X #filling up the original events \n    new_start = len(X)\n    current_idx = len(X)\n    for i in range(len(X)):\n        label = labels[i] \n        multiplier = multipliers[label]\n\n        event = X[i]\n        event_points = event[:-2]\n\n        for j in range(multiplier):\n            theta = np.random.uniform(0, 2 * np.pi) #rotation about the z-axis\n            cos, sin= np.cos(theta), np.sin(theta) #need to get the conversion\n            points_rot = event_points.copy() #don't want to change the original points \n            x, y = points_rot[:,0],points_rot[:,1] #original x and y points \n            points_rot[:,0] = cos * x - sin * y \n            points_rot[:,1] = sin * x + cos * y \n\n            augumented_data[current_idx] = np.concatenate([points_rot, event[-2:]], axis=0)\n            current_idx+=1\n    labels = augumented_data[:,-2,0].astype(int)\n    class_dist = np.array([np.sum(labels==i) for i in range(5)]) #there are 5 labels (0-5) \n    print(f\"The class distribution after augumentation: {class_dist}\")\n\n\n    return augumented_data\n</code></pre>"},{"location":"api/data_limitation/","title":"Data Limitation","text":""},{"location":"api/data_limitation/#data-limitation","title":"Data Limitation","text":"<p>The model we are hoping to train a classification model, hence it is important to balance the classes for the purposes of avoid bias. We can do so simply by ensuring that each class has the same amount of training examples\u2014by limiting them with the class with the lowest events. </p> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p>"},{"location":"api/data_limitation/#scripts.ml_preprocessing_steps.DataLimitation--parameters","title":"Parameters","text":"<p>None</p>"},{"location":"api/data_limitation/#scripts.ml_preprocessing_steps.DataLimitation--return","title":"Return","text":"<p>limited_data: (np.array)     All classes are not balanced through limitation</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>class DataLimitation(BaseEstimator,TransformerMixin):\n    \"\"\"\n    Parameters\n    ----------\n    None\n\n    Return\n    ----------\n    limited_data: (np.array)\n        All classes are not balanced through limitation\n\n    \"\"\"\n\n    def __init__(self, target_size):\n        self.target_size = target_size\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X,y=None):\n        \"\"\"Balancing the classes by limiting to the lowest class to remove any training bias\n\n        Args:\n            X (np.array): data array with reclassified labels\n            y (None): Defaults to None.\n\n        Returns:\n            (np.array): limited data with balanced classes\n        \"\"\"\n        labels = X[:,-2,0].astype(int)\n        valid_mask = labels &lt; 5\n        X_valid = X[valid_mask]\n        labels_valid = labels[valid_mask]\n\n        unique_labels , counts_labels = np.unique(labels_valid, return_counts=True)\n\n        print(f\"Current class distribution is {counts_labels}\")\n        lowest_events = min(counts_labels)\n        limiting_data = np.full((5*lowest_events,self.target_size + 2,4), np.nan)\n        print(f\"The new shape after limiting to lowest class{limiting_data.shape}\")\n\n        counters = [0, 0, 0, 0, 0]\n        insert_index = 0\n\n        for i in range(len(X_valid)):\n            label = labels_valid[i]\n            if counters[label] &lt; lowest_events:\n                limiting_data[insert_index] = X_valid[i] \n                counters[label] += 1\n                insert_index += 1\n\n            if all(c == lowest_events for c in counters):\n                break\n\n        labels_ll = limiting_data[:,-2,0].astype(int)\n        _ , counts_labels_ll = np.unique(labels_ll, return_counts=True)\n        print(f\"Updated class distribution is {counts_labels_ll}\")\n\n        return limiting_data\n</code></pre>"},{"location":"api/data_limitation/#scripts.ml_preprocessing_steps.DataLimitation.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Balancing the classes by limiting to the lowest class to remove any training bias</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>data array with reclassified labels</p> required <code>y</code> <code>None</code> <p>Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>array</code> <p>limited data with balanced classes</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>def transform(self,X,y=None):\n    \"\"\"Balancing the classes by limiting to the lowest class to remove any training bias\n\n    Args:\n        X (np.array): data array with reclassified labels\n        y (None): Defaults to None.\n\n    Returns:\n        (np.array): limited data with balanced classes\n    \"\"\"\n    labels = X[:,-2,0].astype(int)\n    valid_mask = labels &lt; 5\n    X_valid = X[valid_mask]\n    labels_valid = labels[valid_mask]\n\n    unique_labels , counts_labels = np.unique(labels_valid, return_counts=True)\n\n    print(f\"Current class distribution is {counts_labels}\")\n    lowest_events = min(counts_labels)\n    limiting_data = np.full((5*lowest_events,self.target_size + 2,4), np.nan)\n    print(f\"The new shape after limiting to lowest class{limiting_data.shape}\")\n\n    counters = [0, 0, 0, 0, 0]\n    insert_index = 0\n\n    for i in range(len(X_valid)):\n        label = labels_valid[i]\n        if counters[label] &lt; lowest_events:\n            limiting_data[insert_index] = X_valid[i] \n            counters[label] += 1\n            insert_index += 1\n\n        if all(c == lowest_events for c in counters):\n            break\n\n    labels_ll = limiting_data[:,-2,0].astype(int)\n    _ , counts_labels_ll = np.unique(labels_ll, return_counts=True)\n    print(f\"Updated class distribution is {counts_labels_ll}\")\n\n    return limiting_data\n</code></pre>"},{"location":"api/data_strip_id/","title":"Data Stripping","text":""},{"location":"api/data_strip_id/#stripping-data-of-event-id","title":"Stripping Data of Event Id","text":""},{"location":"api/data_strip_id/#scripts.data_extract.prep_4_ml","title":"<code>prep_4_ml(group)</code>","text":"<p>Function to strip data of event names, attach labels to data and convert to .npy files</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>Group</code> <p>HDF5 group object.</p> required <p>Returns:</p> Type Description <p>np.ndarray: Prepared data array for machine learning.</p> Source code in <code>scripts/data_extract.py</code> <pre><code>def prep_4_ml(group):\n    \"\"\"\n    Function to strip data of event names, attach labels to data and convert to .npy files\n\n    Args:\n        group (h5py.Group): HDF5 group object.\n\n\n    Returns: \n        np.ndarray: Prepared data array for machine learning.        \n    \"\"\"\n    attributes = dict(group.attrs)\n    min_event = attributes[\"min_event\"]\n    max_event = attributes[\"max_event\"]\n\n    event_lengths = np.zeros(max_event-min_event, int)\n\n    for i, e in enumerate(group):\n        event_lengths[i] = len(group[e])\n\n    event_data = np.full((len(event_lengths), np.max(event_lengths) + 2, 4), np.nan)\n\n    for i, e in tqdm(enumerate(group)): \n        for n in range(event_lengths[i]):\n            event_data[i, n] = group[e][n,:4]\n        label = group[e].attrs[\"tracks\"]\n        event_data[i, -2] = [label] * 4 # label for classification\n        event_data[i, -1] = [i] * 4 # event index for reference\n\n    return event_lengths, event_data\n</code></pre>"},{"location":"api/inspect/","title":"Inspect","text":""},{"location":"api/inspect/#inspecting-experimental-data","title":"Inspecting Experimental Data","text":"<p>Class for inspecting HDF5 cluster data, checking for NaNs, adding track counts, and visualizing clusters.</p>"},{"location":"api/inspect/#scripts.data_inspect.Inspect--parameters","title":"Parameters","text":"<p>number_to_viz: (int)     Number of events to visualize. num_tracks: (int)     Number of tracks to filter for visualization.</p>"},{"location":"api/inspect/#scripts.data_inspect.Inspect--return","title":"Return","text":"<p>None</p> Source code in <code>scripts/data_inspect.py</code> <pre><code>class Inspect:\n    \"\"\"\n    Class for inspecting HDF5 cluster data, checking for NaNs, adding track counts,\n    and visualizing clusters.\n\n    Parameters\n    ----------\n    number_to_viz: (int)\n        Number of events to visualize.\n    num_tracks: (int)\n        Number of tracks to filter for visualization.\n\n    Return\n    ----------\n    None\n    \"\"\"\n\n    def __init__(self, number_to_viz: int | None, num_tracks: int | None):\n        \"\"\"\n        Initialize the Inspect object.\n\n        Args:\n            number_to_viz (int): Number of events to visualize.\n            num_tracks (int): Number of tracks to filter for visualization.\n        \"\"\"\n        self.number_to_viz = number_to_viz\n        self.num_tracks = num_tracks\n\n    def h5_keys_extract(self, file_h5):\n        \"\"\"\n        Extract the first group from an HDF5 file.\n\n        Args:\n            file_h5 (h5py.File): HDF5 file object.\n\n        Returns:\n            h5py.Group: The first HDF5 group in the file.\n        \"\"\"\n        group_ls = list(file_h5.keys())[0]  # Getting the first group\n        group = file_h5[group_ls]  # Accessing the group\n        return group\n\n    def check_nans(self, group) -&gt; bool:\n        \"\"\"\n        Check for NaN values in each dataset of the HDF5 group.\n\n        Args:\n            group (h5py.Group): HDF5 group object.\n\n        Returns:\n            bool: True if any NaNs are found, False otherwise.\n        \"\"\"\n        for key in tqdm(group, desc=\"Checking for NaNs\"):\n            data_array = group[key][:]\n            if np.isnan(data_array).any():\n                return True  # NaNs found\n        return False  # No NaNs found\n\n    def add_attr_tracks(self, group, file_est: str):\n        \"\"\"\n        Add a new attribute 'tracks' to each dataset in the HDF5 group from parquet files.\n\n        Args:\n            group (h5py.Group): HDF5 group object.\n            est_path (str): Path to the parquet file with estimate classification.\n\n        Returns:\n            h5py.Group: Modified HDF5 group with new 'tracks' attribute.\n        \"\"\"\n        grouped = file_est.groupby(\"event\")\n        group_sizes = grouped.size()\n\n        for event, size in tqdm(group_sizes.items(), total=len(group_sizes), desc=\"Adding class attribute\"):\n            key = \"cloud_\" + str(event)\n            if key in group:\n                group[key].attrs[\"tracks\"] = size\n        return group\n\n    def viz_cluster(self, group) -&gt; None:\n        \"\"\"\n        Visualize clusters in the HDF5 group and save to a PDF.\n\n        Args:\n            group (h5py.Group): HDF5 group object.\n\n        Returns:\n            None\n        \"\"\"\n        attribute = \"tracks\"  # Attribute corresponding to the number of tracks\n        counter = 0  # Counter to limit the number of visualizations\n\n        pdf_path = f\"/Users/pranjalsingh/Desktop/research_space_spyral/AT-TPC_ML_workflow/plots/visualising_{self.num_tracks}_tracks.pdf\"\n        with PdfPages(pdf_path) as pdf:\n            for key in enumerate(group, desc=\"Visualizing clusters\"):\n                if attribute in group[key].attrs:\n                    num_tracks = group[key].attrs[attribute]\n                    if num_tracks == self.num_tracks and counter &lt; self.number_to_viz:\n                        fig = plt.figure()\n                        ax = fig.add_subplot(111, projection='3d')\n\n                        x = group[key][:, 0]\n                        y = group[key][:, 1]\n                        z = group[key][:, 2]\n                        charge = group[key][:, 3]\n\n                        ax.scatter(x / 250, y / 250, (z - 500) / 500, marker='o', s=10)\n                        ax.set_xlabel('X')\n                        ax.set_ylabel('Y')\n                        ax.set_zlabel('Z')\n                        ax.set_xlim(-1, 1)\n                        ax.set_ylim(-1, 1)\n                        ax.set_zlim(-1, 1)\n                        ax.set_title(f'Event {key}')\n\n                        ax.text2D(0.05, 0.95, f'Estimator Data: {num_tracks}', transform=ax.transAxes, color='red')\n\n                        pdf.savefig(fig)\n                        plt.close(fig)\n\n                        counter += 1  # Increment counter\n</code></pre>"},{"location":"api/inspect/#scripts.data_inspect.Inspect.__init__","title":"<code>__init__(number_to_viz, num_tracks)</code>","text":"<p>Initialize the Inspect object.</p> <p>Parameters:</p> Name Type Description Default <code>number_to_viz</code> <code>int</code> <p>Number of events to visualize.</p> required <code>num_tracks</code> <code>int</code> <p>Number of tracks to filter for visualization.</p> required Source code in <code>scripts/data_inspect.py</code> <pre><code>def __init__(self, number_to_viz: int | None, num_tracks: int | None):\n    \"\"\"\n    Initialize the Inspect object.\n\n    Args:\n        number_to_viz (int): Number of events to visualize.\n        num_tracks (int): Number of tracks to filter for visualization.\n    \"\"\"\n    self.number_to_viz = number_to_viz\n    self.num_tracks = num_tracks\n</code></pre>"},{"location":"api/inspect/#scripts.data_inspect.Inspect.add_attr_tracks","title":"<code>add_attr_tracks(group, file_est)</code>","text":"<p>Add a new attribute 'tracks' to each dataset in the HDF5 group from parquet files.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>Group</code> <p>HDF5 group object.</p> required <code>est_path</code> <code>str</code> <p>Path to the parquet file with estimate classification.</p> required <p>Returns:</p> Type Description <p>h5py.Group: Modified HDF5 group with new 'tracks' attribute.</p> Source code in <code>scripts/data_inspect.py</code> <pre><code>def add_attr_tracks(self, group, file_est: str):\n    \"\"\"\n    Add a new attribute 'tracks' to each dataset in the HDF5 group from parquet files.\n\n    Args:\n        group (h5py.Group): HDF5 group object.\n        est_path (str): Path to the parquet file with estimate classification.\n\n    Returns:\n        h5py.Group: Modified HDF5 group with new 'tracks' attribute.\n    \"\"\"\n    grouped = file_est.groupby(\"event\")\n    group_sizes = grouped.size()\n\n    for event, size in tqdm(group_sizes.items(), total=len(group_sizes), desc=\"Adding class attribute\"):\n        key = \"cloud_\" + str(event)\n        if key in group:\n            group[key].attrs[\"tracks\"] = size\n    return group\n</code></pre>"},{"location":"api/inspect/#scripts.data_inspect.Inspect.check_nans","title":"<code>check_nans(group)</code>","text":"<p>Check for NaN values in each dataset of the HDF5 group.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>Group</code> <p>HDF5 group object.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if any NaNs are found, False otherwise.</p> Source code in <code>scripts/data_inspect.py</code> <pre><code>def check_nans(self, group) -&gt; bool:\n    \"\"\"\n    Check for NaN values in each dataset of the HDF5 group.\n\n    Args:\n        group (h5py.Group): HDF5 group object.\n\n    Returns:\n        bool: True if any NaNs are found, False otherwise.\n    \"\"\"\n    for key in tqdm(group, desc=\"Checking for NaNs\"):\n        data_array = group[key][:]\n        if np.isnan(data_array).any():\n            return True  # NaNs found\n    return False  # No NaNs found\n</code></pre>"},{"location":"api/inspect/#scripts.data_inspect.Inspect.h5_keys_extract","title":"<code>h5_keys_extract(file_h5)</code>","text":"<p>Extract the first group from an HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>file_h5</code> <code>File</code> <p>HDF5 file object.</p> required <p>Returns:</p> Type Description <p>h5py.Group: The first HDF5 group in the file.</p> Source code in <code>scripts/data_inspect.py</code> <pre><code>def h5_keys_extract(self, file_h5):\n    \"\"\"\n    Extract the first group from an HDF5 file.\n\n    Args:\n        file_h5 (h5py.File): HDF5 file object.\n\n    Returns:\n        h5py.Group: The first HDF5 group in the file.\n    \"\"\"\n    group_ls = list(file_h5.keys())[0]  # Getting the first group\n    group = file_h5[group_ls]  # Accessing the group\n    return group\n</code></pre>"},{"location":"api/inspect/#scripts.data_inspect.Inspect.viz_cluster","title":"<code>viz_cluster(group)</code>","text":"<p>Visualize clusters in the HDF5 group and save to a PDF.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>Group</code> <p>HDF5 group object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>scripts/data_inspect.py</code> <pre><code>def viz_cluster(self, group) -&gt; None:\n    \"\"\"\n    Visualize clusters in the HDF5 group and save to a PDF.\n\n    Args:\n        group (h5py.Group): HDF5 group object.\n\n    Returns:\n        None\n    \"\"\"\n    attribute = \"tracks\"  # Attribute corresponding to the number of tracks\n    counter = 0  # Counter to limit the number of visualizations\n\n    pdf_path = f\"/Users/pranjalsingh/Desktop/research_space_spyral/AT-TPC_ML_workflow/plots/visualising_{self.num_tracks}_tracks.pdf\"\n    with PdfPages(pdf_path) as pdf:\n        for key in enumerate(group, desc=\"Visualizing clusters\"):\n            if attribute in group[key].attrs:\n                num_tracks = group[key].attrs[attribute]\n                if num_tracks == self.num_tracks and counter &lt; self.number_to_viz:\n                    fig = plt.figure()\n                    ax = fig.add_subplot(111, projection='3d')\n\n                    x = group[key][:, 0]\n                    y = group[key][:, 1]\n                    z = group[key][:, 2]\n                    charge = group[key][:, 3]\n\n                    ax.scatter(x / 250, y / 250, (z - 500) / 500, marker='o', s=10)\n                    ax.set_xlabel('X')\n                    ax.set_ylabel('Y')\n                    ax.set_zlabel('Z')\n                    ax.set_xlim(-1, 1)\n                    ax.set_ylim(-1, 1)\n                    ax.set_zlim(-1, 1)\n                    ax.set_title(f'Event {key}')\n\n                    ax.text2D(0.05, 0.95, f'Estimator Data: {num_tracks}', transform=ax.transAxes, color='red')\n\n                    pdf.savefig(fig)\n                    plt.close(fig)\n\n                    counter += 1  # Increment counter\n</code></pre>"},{"location":"api/ml_training/","title":"Training Pipeline","text":""},{"location":"api/ml_training/#training-with-experimental-data","title":"Training with Experimental Data","text":""},{"location":"api/ml_training/#scripts.ml_training.hyperparameter_tuning","title":"<code>hyperparameter_tuning(batch_size_num, learning_rate_num, cfg_ml_train)</code>","text":"<p>Training with a specific batch_size and learning_rate combination</p> <p>Parameters:</p> Name Type Description Default <code>batch_size_num</code> <code>int</code> <p>batch size </p> required <code>learning_rate_num</code> <code>int</code> <p>learning rate</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>best validation accuracy for the specific combination</p> Source code in <code>scripts/ml_training.py</code> <pre><code>def hyperparameter_tuning(batch_size_num: int,learning_rate_num: int, cfg_ml_train) -&gt; float:\n    \"\"\"\n    Training with a specific batch_size and learning_rate combination\n\n    Args:\n        batch_size_num (int): batch size \n        learning_rate_num (int): learning rate\n\n    Returns:\n        float: best validation accuracy for the specific combination\n    \"\"\"\n    print(f\"\\n&gt;&gt;&gt; Training with batch size = {batch_size_num} and learning rate = {learning_rate_num}\")\n\n    train_features = np.load(cfg_ml_train[\"data_dir\"] + \"16O_size800_train_features.npy\")\n    train_labels = np.load(cfg_ml_train[\"data_dir\"] + \"16O_size800_train_labels.npy\")\n    print(\"Training data shape:\", train_features.shape)\n    train_ds = tf.data.Dataset.from_tensor_slices((train_features[:,:,:3], train_labels))\n    train_ds = train_ds.batch(batch_size=batch_size_num, drop_remainder=False)\n\n    val_features = np.load(cfg_ml_train[\"data_dir\"] + \"16O_size800_val_features.npy\")\n    val_labels = np.load(cfg_ml_train[\"data_dir\"] + \"16O_size800_val_labels.npy\")\n    val_ds = tf.data.Dataset.from_tensor_slices((val_features[:,:,:3], val_labels))\n    val_ds = val_ds.batch(batch_size=batch_size_num, drop_remainder=True)\n\n\n    #early stopping\n    early_stopping = EarlyStopping(\n        monitor = \"val_sparse_categorical_accuracy\",\n        mode = \"max\",\n        patience = 10,\n        restore_best_weights = True,\n    )\n\n    # build and train event-wise classification model and plot learning curve\n    model = create_pointnet_model(num_points=800, \n                          num_classes=5, \n                          num_dimensions=3, #for changing number of features\n                          is_regression=False,\n                          is_pointwise_prediction=False)\n    #model.summary()\n    model.compile(loss=\"sparse_categorical_crossentropy\",\n                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate_num),\n                  metrics=[\"sparse_categorical_accuracy\"])\n\n\n    #checkpointing\n    history = model.fit(train_ds, validation_data=val_ds, epochs=cfg_ml_train[\"epochs_limit\"], callbacks=[early_stopping],verbose=2) #early stopping\n\n    best_val_acc = np.max(history.history[\"val_sparse_categorical_accuracy\"])\n    print(f\"Best val_accuracy={best_val_acc:.4f}\")   \n    return best_val_acc\n</code></pre>"},{"location":"api/ml_training/#scripts.ml_training.load_classfication_model","title":"<code>load_classfication_model(cfg_ml_train)</code>","text":"<p>Evaluating the performance on the model with best hyperparameters </p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>scripts/ml_training.py</code> <pre><code>def load_classfication_model(cfg_ml_train) -&gt; None:\n    \"\"\"\n    Evaluating the performance on the model with best hyperparameters \n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    tf.keras.backend.clear_session() \n    test_features = np.load(cfg_ml_train[\"data_dir\"] + \"16O_size800_test_features.npy\")\n    test_features = test_features[:,:,:3]\n    test_labels = np.load(cfg_ml_train[\"data_dir\"] + \"16O_size800_test_labels.npy\")\n\n    print(\"Shape of test_features:\", test_features.shape)\n\n    model = create_pointnet_model(num_points=800, \n                          num_classes=5, \n                          num_dimensions=3, #for changing number of features\n                          is_regression=False, #no regression\n                          is_pointwise_prediction=False) # we just want classification\n\n    model.compile(loss=\"sparse_categorical_crossentropy\",\n                  optimizer=keras.optimizers.Adam(learning_rate=5e-06),\n                  metrics=[\"sparse_categorical_accuracy\"])\n\n    model.summary()\n    loss, accuracy_d = model.evaluate(test_features,test_labels,verbose=2)\n    print(\"Untrained model, accuracy: {:5.2f}%\".format(100 * accuracy_d))\n    print(\"Unique label values:\", np.unique(test_labels))\n\n    model = tf.keras.models.load_model(cfg_ml_train[\"best_model_path\"])\n    loss, accuracy_d = model.evaluate(test_features,test_labels,verbose=2)\n    print(\"Restored model, accuracy: {:5.2f}%\".format(100 * accuracy_d))\n\n    y_pred = model.predict(test_features)\n    predicted_classes = np.argmax(y_pred, axis=1)\n\n    f1 = f1_score(test_labels, predicted_classes, average='weighted')\n    f1_cl2 = f1_score(test_labels, predicted_classes, labels=[2], average='weighted')\n    cf_matrix = confusion_matrix(test_labels, predicted_classes)\n\n    print(f\"F1 Score: {f1}\")\n    print(f\"F1 Score Class 2: {f1_cl2}\")\n    print(\"Confusion Matrix:\")\n    #print(cm)\n\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n          zip(group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(5,5)\n    class_names = [\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\"]  # Change to your labels\n\n    report = classification_report(test_labels, predicted_classes, target_names=class_names)\n    print(report)\n\n    svm = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Reds')\n    svm.set_xlabel(\"Predicted Labels\")\n    svm.set_ylabel(\"True Labels\")\n    svm.set_xticklabels(class_names)\n    svm.set_yticklabels(class_names, rotation=0) \n    plt.savefig(cfg_ml_train[\"confusion_matrix_path\"])\n</code></pre>"},{"location":"api/ml_training/#scripts.ml_training.run_experiment","title":"<code>run_experiment(cfg_ml_train)</code>","text":"<p>Finding the best batch size and learning rate combination </p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>best parameters (batch_size, learning_rate)</p> Source code in <code>scripts/ml_training.py</code> <pre><code>def run_experiment(cfg_ml_train) -&gt; tuple:\n    \"\"\"\n    Finding the best batch size and learning rate combination \n\n    Args:\n        None\n\n    Returns:\n        tuple: best parameters (batch_size, learning_rate)\n    \"\"\"\n    batch_options = cfg_ml_train[\"batch_options\"]\n    lr_options = cfg_ml_train[\"lr_options\"]\n    results = {}\n\n    for bs, lr in itertools.product(batch_options,lr_options):\n        val_acc = hyperparameter_tuning(bs,lr,cfg_ml_train)\n        results[(bs,lr)] = val_acc\n\n    opt_params = max(results, key=results.get)\n    print(f\"\\n Best parameter pair is {opt_params} with the accuracy of {results[opt_params]}\")\n\n    return opt_params\n</code></pre>"},{"location":"api/ml_training/#scripts.ml_training.train_best_model","title":"<code>train_best_model(cfg_ml_train)</code>","text":"<p>Training the model with the best parameters </p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>scripts/ml_training.py</code> <pre><code>def train_best_model(cfg_ml_train) -&gt; None:\n    \"\"\"\n    Training the model with the best parameters \n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    batch_size_num, learning_rate_num = run_experiment(cfg_ml_train)\n    print(f\"\\n&gt;&gt;&gt; Training with optimised batch size = {batch_size_num} and learning rate = {learning_rate_num}\")\n\n    train_features = np.load(cfg_ml_train[\"data_dir\"] + \"16O_size800_train_features.npy\")\n    train_labels = np.load(cfg_ml_train[\"data_dir\"] + \"16O_size800_train_labels.npy\")\n    print(\"Training data shape:\", train_features.shape)\n    train_ds = tf.data.Dataset.from_tensor_slices((train_features[:,:,:3], train_labels))\n    train_ds = train_ds.batch(batch_size=batch_size_num, drop_remainder=False)\n\n    val_features = np.load(cfg_ml_train[\"data_dir\"] + \"16O_size800_val_features.npy\")\n    val_labels = np.load(cfg_ml_train[\"data_dir\"] + \"16O_size800_val_labels.npy\")\n    val_ds = tf.data.Dataset.from_tensor_slices((val_features[:,:,:3], val_labels))\n    val_ds = val_ds.batch(batch_size=batch_size_num, drop_remainder=True)\n\n    #early stopping\n    early_stopping = EarlyStopping(\n        monitor = \"val_sparse_categorical_accuracy\",\n        mode = \"max\",\n        patience = 10,\n        restore_best_weights = True,\n    )\n\n    best_model_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath= cfg_ml_train[\"best_model_path\"],\n    monitor=\"val_sparse_categorical_accuracy\",\n    mode=\"max\",\n    save_best_only=True,\n    verbose=1,\n)\n\n    # build and train event-wise classification model and plot learning curve\n    model = create_pointnet_model(num_points=800, \n                          num_classes=5, \n                          num_dimensions=3, #for changing number of features\n                          is_regression=False,\n                          is_pointwise_prediction=False)\n\n    model.summary()\n    model.compile(loss=\"sparse_categorical_crossentropy\",\n                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate_num),\n                  metrics=[\"sparse_categorical_accuracy\"])\n\n\n    #checkpointing\n    history = model.fit(train_ds, validation_data=val_ds, epochs=cfg_ml_train[\"epochs_limit\"], callbacks=[early_stopping,best_model_callback],verbose=2) #early stopping\n    plot_learning_curve(history, cfg_ml_train[\"learning_curve_path\"],batch_size_num,learning_rate_num)\n\n    best_epoch = np.argmax(history.history[\"val_sparse_categorical_accuracy\"]) + 1\n    best_val_acc = np.max(history.history[\"val_sparse_categorical_accuracy\"])\n    print(f\"Best model was at epoch {best_epoch} with val_accuracy={best_val_acc:.4f}\") \n</code></pre>"},{"location":"api/outlier_detection/","title":"Outlier Detection","text":""},{"location":"api/outlier_detection/#outlier-detection","title":"Outlier Detection","text":"<p>We can remove any points from the data by the physical dimension of the detectors, <code>[250,250,1000]</code> in x, y, and z, respectively.</p> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p>"},{"location":"api/outlier_detection/#scripts.ml_preprocessing_steps.OutlierDetection--parameters","title":"Parameters","text":"<p>None</p>"},{"location":"api/outlier_detection/#scripts.ml_preprocessing_steps.OutlierDetection--returns","title":"Returns","text":"<p>event_data: (array)     Data with outliers removed event_lengths: (array)     New event lengths with removal of outiler points</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>class OutlierDetection(BaseEstimator,TransformerMixin):\n    \"\"\"\n    Parameters\n    ----------\n    None\n\n    Returns\n    ----------\n    event_data: (array)\n        Data with outliers removed\n    event_lengths: (array)\n        New event lengths with removal of outiler points\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X,y=None):\n        \"\"\"Detecting outliers and removing them from the point cloud data\n\n        Args:\n            X (tuple): utliers removed data with new lengths (event_data,new_event_lengths)\n            y (None): Defaults to None.\n\n        Returns:\n            (tuple): modified data and new event lengths\n        \"\"\"\n        data,event_lengths = X\n        event_data = np.full(data.shape, np.nan)\n        new_event_lengths = np.full_like(event_lengths, np.nan)\n        tot_count = 0\n\n        for i in tqdm.tqdm(range(len(data)), desc=\"Removing outliers\"):\n            event_points = data[i,:event_lengths[i]]\n            condition = ((-270 &lt;= event_points[:, 0]) &amp; (event_points[:, 0] &lt;= 270) &amp;   \\\n                (-270 &lt;= event_points[:, 1]) &amp; (event_points[:, 1] &lt;= 270) &amp;\n                (0 &lt;= event_points[:, 2]) &amp; (event_points[:, 2]  &lt;= 1003))\n            allowed_points = event_points[condition] #only allows points that are not outliers\n\n            event_data[i,:len(allowed_points)] = allowed_points #only assigns the valid points to the new array\n            event_data[i,-2] = data[i,-2] #need to include the labels\n            event_data[i,-1] = data[i,-1] #need to include the original index\n\n            new_event_lengths[i] = len(allowed_points)  #original event number minus the number of outliers\n            tot_count+=event_lengths[i] -new_event_lengths[i]\n\n        print(f\"Number of outlier points removed: {tot_count}\") \n        return (event_data,new_event_lengths)\n</code></pre>"},{"location":"api/outlier_detection/#scripts.ml_preprocessing_steps.OutlierDetection.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Detecting outliers and removing them from the point cloud data</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>tuple</code> <p>utliers removed data with new lengths (event_data,new_event_lengths)</p> required <code>y</code> <code>None</code> <p>Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>modified data and new event lengths</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>def transform(self,X,y=None):\n    \"\"\"Detecting outliers and removing them from the point cloud data\n\n    Args:\n        X (tuple): utliers removed data with new lengths (event_data,new_event_lengths)\n        y (None): Defaults to None.\n\n    Returns:\n        (tuple): modified data and new event lengths\n    \"\"\"\n    data,event_lengths = X\n    event_data = np.full(data.shape, np.nan)\n    new_event_lengths = np.full_like(event_lengths, np.nan)\n    tot_count = 0\n\n    for i in tqdm.tqdm(range(len(data)), desc=\"Removing outliers\"):\n        event_points = data[i,:event_lengths[i]]\n        condition = ((-270 &lt;= event_points[:, 0]) &amp; (event_points[:, 0] &lt;= 270) &amp;   \\\n            (-270 &lt;= event_points[:, 1]) &amp; (event_points[:, 1] &lt;= 270) &amp;\n            (0 &lt;= event_points[:, 2]) &amp; (event_points[:, 2]  &lt;= 1003))\n        allowed_points = event_points[condition] #only allows points that are not outliers\n\n        event_data[i,:len(allowed_points)] = allowed_points #only assigns the valid points to the new array\n        event_data[i,-2] = data[i,-2] #need to include the labels\n        event_data[i,-1] = data[i,-1] #need to include the original index\n\n        new_event_lengths[i] = len(allowed_points)  #original event number minus the number of outliers\n        tot_count+=event_lengths[i] -new_event_lengths[i]\n\n    print(f\"Number of outlier points removed: {tot_count}\") \n    return (event_data,new_event_lengths)\n</code></pre>"},{"location":"api/plotting/","title":"Plotting Learning Curve","text":""},{"location":"api/plotting/#plotting-learning-curve","title":"Plotting Learning Curve","text":"<p>This function plots the learning curves from the ML training. </p> <p>Plotting the learning curve</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>History</code> <p>History object from model.fit</p> required <code>filename</code> <code>str</code> <p>path for saving the learning curve</p> required <code>batch_size</code> <code>int</code> <p>batch size</p> required <code>learning_rate</code> <code>int</code> <p>learning rate</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>scripts/plotting.py</code> <pre><code>def plot_learning_curve(history, filename: str,batch_size: int,learning_rate: int):\n    \"\"\"Plotting the learning curve\n\n    Args:\n        history (keras.callbacks.History): History object from model.fit\n        filename (str): path for saving the learning curve\n        batch_size (int): batch size\n        learning_rate (int): learning rate\n\n    Returns:\n        None\n    \"\"\"\n    plt.figure(figsize=(11, 6), dpi=100)\n    plt.plot(np.array(history.history['loss'])/batch_size, 'o-', label='Training Loss')\n    plt.plot(np.array(history.history['val_loss'])/batch_size, 'o:', color='r', label='Validation Loss')\n    plt.legend(loc='best')\n    plt.title('Learning Curve')\n    plt.suptitle(f\"Batch size: {batch_size}, Learning rate: {learning_rate}\", y=0.8)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.xticks(range(0, len(history.history['loss']), 10), range(1, len(history.history['loss']) + 1, 10))\n    plt.yscale('log')\n    plt.tight_layout()\n    plt.savefig(filename) \n</code></pre>"},{"location":"api/pointnet/","title":"PointNet Architecture","text":"<ul> <li> <p>PointNet paper here</p> </li> <li> <p>Adapted from this example reference</p> </li> </ul> <p>Author: Emilio Villasana, Andrew Rice, Raghu Ramanujan, Dylan Sparks</p>"},{"location":"api/pointnet/#scripts.pointnet.OrthogonalRegularizer","title":"<code>OrthogonalRegularizer</code>","text":"<p>               Bases: <code>Regularizer</code></p> <p>A reimplementation of orthogonal regularization. This incentivizes the rows of  the matrix to be orthogonal to each other.</p> <p>Is not directly replaceable with the tensorflow implementation of this regularizer because this version regularizes matrix shaped output from a layer.</p> Source code in <code>scripts/pointnet.py</code> <pre><code>@tf.keras.utils.register_keras_serializable(package='Custom', name='OrthogonalRegularizer')\nclass OrthogonalRegularizer(keras.regularizers.Regularizer):\n    \"\"\"\n    A reimplementation of orthogonal regularization. This incentivizes the rows of \n    the matrix to be orthogonal to each other.\n\n    Is not directly replaceable with the tensorflow implementation of this regularizer\n    because this version regularizes matrix shaped output from a layer.\n    \"\"\"\n    def __init__(self, num_features, l2reg=0.001):\n        self.num_features = num_features\n        self.l2reg = l2reg\n\n    def __call__(self, x):\n        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n        xxt = tf.tensordot(x, x, axes=(2, 2))\n        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n        return tf.reduce_sum(self.l2reg * tf.square(xxt - tf.eye(self.num_features)))\n\n    def get_config(self):\n        return {'l2reg': self.l2reg,\n                'num_features': self.num_features}\n</code></pre>"},{"location":"api/pointnet/#scripts.pointnet.create_event_wise_head","title":"<code>create_event_wise_head(global_features, num_classes, is_regression)</code>","text":"<p>Implements classification layers from Fig. 2 (but final head could be regression)</p> Source code in <code>scripts/pointnet.py</code> <pre><code>def create_event_wise_head(global_features, num_classes, is_regression):\n    \"\"\" Implements classification layers from Fig. 2 (but final head could be regression) \"\"\"\n    x = dense_bn(global_features, 512)\n    x = layers.Dropout(0.3)(x)\n    x = dense_bn(x, 256)\n    x = layers.Dropout(0.3)(x)\n\n    if not is_regression:\n        return layers.Dense(num_classes, activation=\"softmax\")(x)\n    return layers.Dense(1, activation=\"linear\")(x)    \n</code></pre>"},{"location":"api/pointnet/#scripts.pointnet.create_point_wise_head","title":"<code>create_point_wise_head(local_features, global_features, num_points, num_classes, is_regression)</code>","text":"<p>Implements segmentation layers from Fig. 2 (the yellow shaded region)</p> Source code in <code>scripts/pointnet.py</code> <pre><code>def create_point_wise_head(local_features, global_features, num_points, num_classes, is_regression):\n    \"\"\" Implements segmentation layers from Fig. 2 (the yellow shaded region) \"\"\"\n    x = tf.expand_dims(global_features, axis=1)\n    x = tf.repeat(x, repeats=num_points, axis=1)\n    concat_features = layers.Concatenate(axis=2)([local_features, x])\n    x = conv_bn(concat_features, 512)\n    x = conv_bn(x, 256)\n    x = conv_bn(x, 128)\n    x = layers.Dropout(0.3)(x)\n    x = conv_bn(x, 128)\n    x = layers.Dropout(0.3)(x)\n\n    if not is_regression:\n        return layers.Dense(num_classes, activation=\"softmax\")(x)\n    return layers.Dense(1, activation=\"linear\")(x)\n</code></pre>"},{"location":"api/pointnet/#scripts.pointnet.create_shared_layers","title":"<code>create_shared_layers(inputs, num_dimensions)</code>","text":"<p>Implements shared layers from Fig. 2</p> Source code in <code>scripts/pointnet.py</code> <pre><code>def create_shared_layers(inputs, num_dimensions):\n    \"\"\" Implements shared layers from Fig. 2 \"\"\"\n    x = tnet(inputs, num_dimensions)\n    x = conv_bn(x, 64)\n    x = conv_bn(x, 64)\n    local_features = tnet(x, 64)\n    x = conv_bn(local_features, 64)\n    x = conv_bn(x, 128)\n    x = conv_bn(x, 1024)\n    global_features = layers.GlobalMaxPooling1D()(x)\n\n    return local_features, global_features\n</code></pre>"},{"location":"api/pointnet/#scripts.pointnet.tnet","title":"<code>tnet(inputs, num_features)</code>","text":"<p>Layer sizes are from Appendix C of the PointNet paper</p> Source code in <code>scripts/pointnet.py</code> <pre><code>def tnet(inputs, num_features):\n    \"\"\" Layer sizes are from Appendix C of the PointNet paper \"\"\"\n    x = conv_bn(inputs, 64)\n    x = conv_bn(x, 128)\n    x = conv_bn(x, 1024)\n    x = layers.GlobalMaxPooling1D()(x)\n    x = dense_bn(x, 512)\n    x = dense_bn(x, 256)\n\n    # Initialise bias as the identity matrix (based on section C from Appendix)\n    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n\n    # Incentivize rows of matrix to be orthogonal (Eq. 2 from page 4)\n    reg = OrthogonalRegularizer(num_features)\n\n    x = layers.Dense(num_features * num_features,\n                     kernel_initializer=\"zeros\",\n                     bias_initializer=bias,\n                     activity_regularizer=reg)(x)\n\n    feat_T = layers.Reshape((num_features, num_features))(x)\n    return layers.Dot(axes=(2, 1))([inputs, feat_T])\n</code></pre>"},{"location":"api/reclassifying_labels/","title":"Reclassifying Labels","text":""},{"location":"api/reclassifying_labels/#reclassifying-labels","title":"Reclassifying Labels","text":"<p>For classification ML models, the labels begin with <code>0</code>, for our data that would represent 0 number of tracks. For this reason though the experimental labels begin with 1, we subtract 1 from them for ML purposes. </p> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p>"},{"location":"api/reclassifying_labels/#scripts.ml_preprocessing_steps.ReclassifyingLabels--parameters","title":"Parameters","text":"<p>None</p>"},{"location":"api/reclassifying_labels/#scripts.ml_preprocessing_steps.ReclassifyingLabels--return","title":"Return","text":"<p>X_copy: (np.array)      Labels recalculated but the same data shape remains same  (run_events, target_size,4)</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>class ReclassifyingLabels(BaseEstimator,TransformerMixin):\n    \"\"\"\n    Parameters\n    ----------\n    None\n\n    Return\n    ----------\n    X_copy: (np.array) \n        Labels recalculated but the same data shape remains same  (run_events, target_size,4) \n\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def fit(self,X,y=None):\n        return self \n\n    def transform(self, X,y=None):\n        \"\"\"Reclassifying the labels from number of tracks to classes by subtracting 1\n\n        Args:\n            X (np.array): data array with shape (run_events, target_size,4)\n            y (None): Defaults to None.\n\n        Returns:\n            (np.array): reclassified data labels\n        \"\"\"\n        X_copy = X.copy() #don't want to change the labels from the original\n        for i in range(len(X_copy)):\n            if X_copy[i,-2,0] == 0:\n                print(\"Event has 0 tracks\")\n                print(\"Event:\",i,\" label:\", X_copy[i,-2,0])\n            else:\n                X_copy[i,-2,0] -=1 \n\n        return X_copy\n</code></pre>"},{"location":"api/reclassifying_labels/#scripts.ml_preprocessing_steps.ReclassifyingLabels.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Reclassifying the labels from number of tracks to classes by subtracting 1</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>data array with shape (run_events, target_size,4)</p> required <code>y</code> <code>None</code> <p>Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>array</code> <p>reclassified data labels</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>def transform(self, X,y=None):\n    \"\"\"Reclassifying the labels from number of tracks to classes by subtracting 1\n\n    Args:\n        X (np.array): data array with shape (run_events, target_size,4)\n        y (None): Defaults to None.\n\n    Returns:\n        (np.array): reclassified data labels\n    \"\"\"\n    X_copy = X.copy() #don't want to change the labels from the original\n    for i in range(len(X_copy)):\n        if X_copy[i,-2,0] == 0:\n            print(\"Event has 0 tracks\")\n            print(\"Event:\",i,\" label:\", X_copy[i,-2,0])\n        else:\n            X_copy[i,-2,0] -=1 \n\n    return X_copy\n</code></pre>"},{"location":"api/scaling_data/","title":"Scaling Data","text":""},{"location":"api/scaling_data/#scaling-data","title":"Scaling Data","text":"<p>All features are scaled by <code>MinMaxScaler()</code>, documentation from the scikit-learn library can be found here</p> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p>"},{"location":"api/scaling_data/#scripts.ml_preprocessing_steps.ScalingData--parameters","title":"Parameters","text":"<p>None</p>"},{"location":"api/scaling_data/#scripts.ml_preprocessing_steps.ScalingData--return","title":"Return","text":"<p>X: (np.array)      MinMaxScaler() applied data for all columns</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>class ScalingData(BaseEstimator,TransformerMixin):\n    \"\"\"\n    Parameters\n    ----------\n    None\n\n    Return\n    ----------\n    X: (np.array) \n        MinMaxScaler() applied data for all columns\n\n    \"\"\"\n    def __init__(self,dimension=4):\n        self.dimension = dimension\n        self.scalers = [MinMaxScaler(feature_range=(-1, 1)) for _ in range(dimension)]\n\n    def fit(self,X,y=None):\n        for n in range(self.dimension):\n            data = X[:, :-2, n].reshape(-1, 1)\n            self.scalers[n].fit(data)\n        return self\n\n    def transform(self,X,y=None):\n        \"\"\"Scaling with MinMaxScaler to (-1,1) range\n\n        Args:\n            X (np.array): data array\n            y (None): Defaults to None\n\n        Returns:\n            (np.array): data with scaled training features\n        \"\"\"\n        n_dict = {0:\"x\",1:\"y\",2:\"z\",3:\"charge\"}\n        for n in range(self.dimension):\n            data = X[:, :-2, n].reshape(-1, 1)\n            X[:, :-2, n] = self.scalers[n].transform(data).reshape(X.shape[0], X.shape[1]-2)\n            print(f\"Scaler min/max for {n_dict[n]}: {self.scalers[n].data_min_[0]}, {self.scalers[n].data_max_[0]}\")\n\n        return X\n</code></pre>"},{"location":"api/scaling_data/#scripts.ml_preprocessing_steps.ScalingData.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Scaling with MinMaxScaler to (-1,1) range</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>data array</p> required <code>y</code> <code>None</code> <p>Defaults to None</p> <code>None</code> <p>Returns:</p> Type Description <code>array</code> <p>data with scaled training features</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>def transform(self,X,y=None):\n    \"\"\"Scaling with MinMaxScaler to (-1,1) range\n\n    Args:\n        X (np.array): data array\n        y (None): Defaults to None\n\n    Returns:\n        (np.array): data with scaled training features\n    \"\"\"\n    n_dict = {0:\"x\",1:\"y\",2:\"z\",3:\"charge\"}\n    for n in range(self.dimension):\n        data = X[:, :-2, n].reshape(-1, 1)\n        X[:, :-2, n] = self.scalers[n].transform(data).reshape(X.shape[0], X.shape[1]-2)\n        print(f\"Scaler min/max for {n_dict[n]}: {self.scalers[n].data_min_[0]}, {self.scalers[n].data_max_[0]}\")\n\n    return X\n</code></pre>"},{"location":"api/uniform_noise/","title":"Uniform Noise Addition","text":""},{"location":"api/uniform_noise/#uniform-noise-addition","title":"Uniform Noise Addition","text":""},{"location":"api/uniform_noise/#only-for-use-with-simulated-data","title":"Only for use with simulated data","text":"<p>This method is used only when training with simulated data.</p> <p>Can be used to add uniform noise by using <code>np.random.randint()</code> between <code>[-250,250]</code> for x and y, and <code>[0,1000]</code> for z. </p> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Adds uniform noise to the point cloud data in cartesian coordinates.</p>"},{"location":"api/uniform_noise/#scripts.ml_preprocessing_steps.UniformNoiseAddition--parameters","title":"Parameters","text":"<p>ratio_noise : (float)     Fraction of noise to length of point cloud to be produced</p>"},{"location":"api/uniform_noise/#scripts.ml_preprocessing_steps.UniformNoiseAddition--returns","title":"Returns","text":"<p>new_data: (array)     Data with uniform noise added event_lengths: (array)     New event lengths accounting for added noise points</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>class UniformNoiseAddition(BaseEstimator,TransformerMixin):\n    \"\"\"\n    Adds uniform noise to the point cloud data in cartesian coordinates.\n\n    Parameters\n    ----------\n    ratio_noise : (float)\n        Fraction of noise to length of point cloud to be produced\n\n    Returns\n    ----------\n    new_data: (array)\n        Data with uniform noise added\n    event_lengths: (array)\n        New event lengths accounting for added noise points \n    \"\"\"\n    def __init__(self, ratio_noise: float):\n        self.ratio_noise = ratio_noise\n\n\n    def fit(self,X,y=None):\n        return self  \n\n    def transform(self,X,y=None):\n        \"\"\"Adding uniform noise in cartesian coordinates\n\n        Args:\n            X (tuple):Packed data and event lengths np.array\n            y (None): Defaults to None.\n\n        Returns:\n            (tuple): Data with noise and new event lengths\n        \"\"\"\n        data,event_lengths = X\n        skipped = 0\n        for i in range(len(data)): \n            data_size = int((self.ratio_noise)*event_lengths[i])\n\n            noise_x = np.random.randint(-250,250,(data_size,1))\n            noise_y = np.random.randint(-250,250,(data_size,1))\n            noise_z = np.random.randint(0,1000,(data_size,1))\n\n            array_charge = np.zeros((data_size,1))\n            noise_data = np.concatenate((noise_x,noise_y,noise_z,array_charge),axis=1)\n            combined_data = np.concatenate((data[i,:event_lengths[i],:],noise_data,data[i,-2:,:]),axis=0)\n\n            if combined_data.shape[0] &gt; data.shape[1]:\n                skipped+=1\n                continue\n\n            data[i, :combined_data.shape[0], :] = combined_data\n\n            event_lengths[i]+=data_size\n\n\n        print(f\"Number of events skipped: {skipped}\")\n        return (data,event_lengths)\n</code></pre>"},{"location":"api/uniform_noise/#scripts.ml_preprocessing_steps.UniformNoiseAddition.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Adding uniform noise in cartesian coordinates</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>tuple</code> <p>Packed data and event lengths np.array</p> required <code>y</code> <code>None</code> <p>Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Data with noise and new event lengths</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>def transform(self,X,y=None):\n    \"\"\"Adding uniform noise in cartesian coordinates\n\n    Args:\n        X (tuple):Packed data and event lengths np.array\n        y (None): Defaults to None.\n\n    Returns:\n        (tuple): Data with noise and new event lengths\n    \"\"\"\n    data,event_lengths = X\n    skipped = 0\n    for i in range(len(data)): \n        data_size = int((self.ratio_noise)*event_lengths[i])\n\n        noise_x = np.random.randint(-250,250,(data_size,1))\n        noise_y = np.random.randint(-250,250,(data_size,1))\n        noise_z = np.random.randint(0,1000,(data_size,1))\n\n        array_charge = np.zeros((data_size,1))\n        noise_data = np.concatenate((noise_x,noise_y,noise_z,array_charge),axis=1)\n        combined_data = np.concatenate((data[i,:event_lengths[i],:],noise_data,data[i,-2:,:]),axis=0)\n\n        if combined_data.shape[0] &gt; data.shape[1]:\n            skipped+=1\n            continue\n\n        data[i, :combined_data.shape[0], :] = combined_data\n\n        event_lengths[i]+=data_size\n\n\n    print(f\"Number of events skipped: {skipped}\")\n    return (data,event_lengths)\n</code></pre>"},{"location":"api/up-down_scaling/","title":"Up-Down Scaling","text":""},{"location":"api/up-down_scaling/#resampling","title":"Resampling","text":"<p>Each event has a different amount of point cloud lengths, making the data tensor filled with empty zeros where the point clouds are shorter. We require a static tensor with no zeros, the best way to do this is by choosing a target value\u2014upscaling any events that are lower and downscaling any that are higher.</p> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p>"},{"location":"api/up-down_scaling/#scripts.ml_preprocessing_steps.UpDownScaling--parameters","title":"Parameters","text":"<p>target_size: (int)      The number of points to up/down sample to </p>"},{"location":"api/up-down_scaling/#scripts.ml_preprocessing_steps.UpDownScaling--returns","title":"Returns","text":"<p>new_data: (array)     Up/down sampled data with shape (run_events, target_size,4)</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>class UpDownScaling(BaseEstimator,TransformerMixin):\n    \"\"\"\n    Parameters\n    ----------\n    target_size: (int) \n        The number of points to up/down sample to \n\n    Returns\n    ----------\n    new_data: (array)\n        Up/down sampled data with shape (run_events, target_size,4)\n    \"\"\"\n    def __init__(self,target_size: int,isotope: str,dimension: int = 4):\n        self.target_size = target_size\n        self.pcloud_zeros = 0 #count if there are zero points in an event\n        self.dimension = dimension \n        self.isotope = isotope\n\n    def fit(self,X,y=None):\n        return self \n\n    def transform(self,X,y=None): #for up/down scaling\n        \"\"\"Resampling point clouds to a target value for a static array\n\n        Args:\n            X (tuple): data and event lengths np.array\n            y (None): Defaults to None.\n\n        Returns:\n            (np.array): new data with modified shape\n        \"\"\"\n        data,event_lengths = X #with shape (file,event_lenghts) X needs to be the only input to preserve the conventions of custom transformer\n        len_run = len(data)\n        # new_array_name = isotope + '_size' + str(sample_size) + '_sampled'\n        new_data = np.full((len_run, self.target_size+2, self.dimension), np.nan) \n\n        for i in tqdm.tqdm(range(len_run), desc=\"Resampling data\"): #\n            ev_len = event_lengths[i] #length of event-- i.e. number of instances\n            if ev_len == 0: #if event length is 0\n                print(f\"This event has 0 length: {i}\")\n                self.pcloud_zeros+=1\n                continue\n            if ev_len &gt; self.target_size: #upsample\n                random_points = np.random.choice(ev_len, self.target_size, replace=False)  #choosing the random instances to sample\n                for r in range(len(random_points)):  # #only adds random sample_size points \n                    new_data[i,r] = data[i,random_points[r]]\n\n            else:\n                new_data[i,:ev_len,:] = data[i,:ev_len,:] #downsample\n                need = self.target_size - ev_len\n                random_points = np.random.choice(ev_len, need, replace= True if need &gt; ev_len else False) #only repeats points more points needed than event length \n                count = ev_len\n                for r in random_points:\n                    new_data[i,count] = data[i,r]\n                    if np.isnan(new_data[i, count, 0]):\n                        print(f\"NaN found at event {i}, index {count}\") #need to make sure no nans remain\n                    count += 1\n            new_data[i,-2] = data[i,-2] # saving the label\n            new_data[i,-1] = data[i,-1] # saving the event index\n\n\n        assert self.pcloud_zeros == 0, \"There are events with no points\"\n        assert new_data.shape == (len_run, self.target_size+2, self.dimension), 'Array has incorrect shape'\n        assert len(np.unique(new_data[:,-1,0]))+self.pcloud_zeros == len_run, 'Array has incorrect number of events'\n        assert not np.isnan(new_data).any(), \"NaNs detected in new_data\" #very imporant to make sure there are no nans \n        print(f\"Transformed shape of data: {new_data.shape}\")\n        return new_data\n</code></pre>"},{"location":"api/up-down_scaling/#scripts.ml_preprocessing_steps.UpDownScaling.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Resampling point clouds to a target value for a static array</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>tuple</code> <p>data and event lengths np.array</p> required <code>y</code> <code>None</code> <p>Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>array</code> <p>new data with modified shape</p> Source code in <code>scripts/ml_preprocessing_steps.py</code> <pre><code>def transform(self,X,y=None): #for up/down scaling\n    \"\"\"Resampling point clouds to a target value for a static array\n\n    Args:\n        X (tuple): data and event lengths np.array\n        y (None): Defaults to None.\n\n    Returns:\n        (np.array): new data with modified shape\n    \"\"\"\n    data,event_lengths = X #with shape (file,event_lenghts) X needs to be the only input to preserve the conventions of custom transformer\n    len_run = len(data)\n    # new_array_name = isotope + '_size' + str(sample_size) + '_sampled'\n    new_data = np.full((len_run, self.target_size+2, self.dimension), np.nan) \n\n    for i in tqdm.tqdm(range(len_run), desc=\"Resampling data\"): #\n        ev_len = event_lengths[i] #length of event-- i.e. number of instances\n        if ev_len == 0: #if event length is 0\n            print(f\"This event has 0 length: {i}\")\n            self.pcloud_zeros+=1\n            continue\n        if ev_len &gt; self.target_size: #upsample\n            random_points = np.random.choice(ev_len, self.target_size, replace=False)  #choosing the random instances to sample\n            for r in range(len(random_points)):  # #only adds random sample_size points \n                new_data[i,r] = data[i,random_points[r]]\n\n        else:\n            new_data[i,:ev_len,:] = data[i,:ev_len,:] #downsample\n            need = self.target_size - ev_len\n            random_points = np.random.choice(ev_len, need, replace= True if need &gt; ev_len else False) #only repeats points more points needed than event length \n            count = ev_len\n            for r in random_points:\n                new_data[i,count] = data[i,r]\n                if np.isnan(new_data[i, count, 0]):\n                    print(f\"NaN found at event {i}, index {count}\") #need to make sure no nans remain\n                count += 1\n        new_data[i,-2] = data[i,-2] # saving the label\n        new_data[i,-1] = data[i,-1] # saving the event index\n\n\n    assert self.pcloud_zeros == 0, \"There are events with no points\"\n    assert new_data.shape == (len_run, self.target_size+2, self.dimension), 'Array has incorrect shape'\n    assert len(np.unique(new_data[:,-1,0]))+self.pcloud_zeros == len_run, 'Array has incorrect number of events'\n    assert not np.isnan(new_data).any(), \"NaNs detected in new_data\" #very imporant to make sure there are no nans \n    print(f\"Transformed shape of data: {new_data.shape}\")\n    return new_data\n</code></pre>"},{"location":"setup/data_inspect_info/","title":"Data Inspect","text":"<p>This page will expand on the four methods given by the <code>Inspect</code> class and how some methods can be used individually to inspect the pointclouds. </p> <p>Perhaps first we can briefly discuss the methods that we do used to attach the Pointcloud data to labels.</p> <p><pre><code>h5_keys_extract(self, file_h5)\n</code></pre> This is simply used to return hdf5 keys for an h5 file. Note: This only works for <code>.h5</code> files formatted through the Spyral algorithm, and not just any <code>.h5</code> file.</p> <p>Next,</p> <p><pre><code>add_attr_tracks(self, group, file_est: str)\n</code></pre> is used to return hdf5 keys with an added attribute corresponding to the label of that event from the Estimation phase\u2014where each key has its own label. </p> <p>On the other hand,</p> <p><pre><code>check_nans(self, group)\n</code></pre> allows one to verify that there are indeed no NaNs within our dataset after the Spyral analysis. </p> <p>Now, these are all nice, but sometimes we need to visualize the Pointcloud, and we can do so with the use of </p> <p><pre><code>viz_cluster(self, group)\n</code></pre> This will create a PDF file with the amount of tracks specified by the user (with the <code>number_to_viz</code> parameter), events with only a specific number of tracks (using the <code>num_tracks</code>). Note: Ensure to pass <code>None</code> for those parameters if this method is not being used.</p>"},{"location":"setup/environment/","title":"Environment Setup","text":""},{"location":"setup/environment/#virtual-environment-setup-spyral","title":"Virtual Environment Setup: Spyral","text":"<p>The first step of the ML workflow involves extracting training features and labels, we do so from a package written by many individuals in the AT-TPC group called Spyral, which has its own documentation page. </p> <p>However, for our purposes, the steps outlined below will be in a condensed form. </p> <p>First navigate to the folder on your local or High Performance Cluster (recommended) you would want to be in and clone this repository with</p> <pre><code>git clone &lt;repository_url&gt;\n</code></pre> <p>then </p> <pre><code>cd AT-TPC_ML_workflow\n</code></pre> <p>We will now be installing the AT-TPC package but do not want to do so globally, instead we will first create a virtual environment.</p> <p>This package only works for a Python version with in the bounds of &gt; 3.10 and &lt; 3.13 (either 3.11 or 3.12), therefore we must ensure the environment is created with the right version.  </p> <p>You can check you current version by </p> <p><pre><code>python --version\n</code></pre> And then run this command to create the virtual environment</p> <p><pre><code>python3.12 -m venv .venv\n</code></pre> Or if you are doing this on an HPC, load the correct python version before you activate this environment. This process could be specific to the HPC you use, and you may need to refer to their specific documentation. As an example, for my specific cluster I use <pre><code>module load Python/3.12.3-GCCcore-13.3.0\n</code></pre> then run <pre><code>python -m venv .venv\n</code></pre></p> <p>Note that it is important you run this command inside the AT-TPC_ML_workflow folder. Now we will activate this environment by </p> <pre><code>source .venv/bin/activate\n</code></pre> <p>and finally install attpc_spyral and its dependencies with a <code>requirements.txt</code>, a file that should be part of the GitHub clone. If you inspect this file, all the dependencies can be seen. </p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Congratulations! This installs your virtual environment for the training features and labels extraction. You can close this environment with </p> <pre><code>deactivate\n</code></pre> <p>Please do this before proceeding to the next step.</p>"},{"location":"setup/environment/#conda-environment-setup-ml","title":"Conda Environment Setup: ML","text":"<p>Unlike the virtual environment for ML training extraction, we will use a Conda environment for the ML side of things, this includes the preprocessing for ML data. </p> <p>This environment requires more rigid dependencies' versions than the virtual environment we installed in the step above, hence why we will be using a file named environment.yml to install all the dependencies with their receptive versions. </p> <p>Simply run <pre><code>conda env create -f environment.yml\n</code></pre></p> <p>If you inspect this file, it has all the right dependencies you would need for running ML side of this project, and will create a Conda environment called \"tf_Jul2025\".</p> <p>You can exit this environment by similar command to the virtual environment </p> <pre><code>conda deactivate\n</code></pre>"},{"location":"setup/file_change_data_extract/","title":"Data Extraction","text":""},{"location":"setup/file_change_data_extract/#configuration","title":"Configuration","text":"<p>Okay, you have set up the configuration for the Spyral files, but what's next? If you inspect <code>config.json</code>, you can see that there are other configurations we will have to set before we can run this in a workflow, or individually extract the data and labels. </p> <p>The parameters that correspond to extracting the point clouds with their labels is <code>data_extract_parameters</code>, and it consists of:</p> <pre><code>\"run_min\": 54,\n\"run_max\": 169,\n\"file_path\": \"path/to/pointcloud/h5/file\",\n\"est_path\": \"path/to/est/file\"   \n</code></pre> <p>Where <code>run_min</code> and <code>run_max</code> exactly how many runs for which to get the data; though they are no different that the same parameters in <code>spyral_parameters</code>, however, one may choose not to process all the data together. This allows for freedom in how much data we want to train on (most likely not going to be all of it). </p> <p>The <code>file_path</code> is going to be for the Pointcloud directory, the point cloud files from which we will extract the features; and <code>est_path</code> for the Estimation directory, from which we extract the labels. </p> <p>Congratulations! Now you will be able to get data that has the labels attached to it and simply need to run. </p> <pre><code>python fnc_all_runs.py\n</code></pre>"},{"location":"setup/file_change_ml/","title":"ML","text":"<p>The machine learning (ML) configuration parameters are divided into two sets, one corresponding to the preprocessing pipeline\u2014how we prepare our data for training, and the ML model training and evaluation. </p> <p>Let's begin our journey of specifying parameters once again, but I promise we're done soon!</p>"},{"location":"setup/file_change_ml/#ml-preprocessing-pipeline","title":"ML Preprocessing Pipeline","text":""},{"location":"setup/file_change_ml/#configuration","title":"Configuration","text":"<p>There are few parameters that will familiar to us, but also some new ones that are specific to ML preprocessing.</p> <pre><code>\"target_size\": 800,\n\"isotope\": \"O16\",\n\"run_min\": 54,\n\"run_max\": 169,\n\"data\": \"path/to/processed/pointcloud/data/\",\n\"event_lengths\": \"path/to/processed/event/lengths/\",\n\"directory_training\": \"path/to/save/ml/training/data/\"\n</code></pre> <p><code>run_min</code> and <code>run_max</code> are redefined here to give the user the ability to have more freedom over what exact runs they would like to run the ML preprocessing pipeline on, it is important to note that one can set these to the same values in each configuration step if they choose to. The previous data extraction step writes two <code>.npy</code> files for each run (event_lengths and pointcloud data), the <code>event_lengths</code> and <code>data</code> parameters refer to their directory path respectively; where the <code>directory_training</code> is the path to the directory where the data from this pipeline will be saved.</p> <p>To understand what value to set <code>target_size</code> to, it's important to learn of it when placed in the context of the up/downscaling section. In summary, our ML model requires static arrays to train on, however, each event in our data can have a different number of points, therefore, we require a way for all events to same amount of points. To do this, we set a target size that we will reach either by adding points or removing them; after some brief research I have found <code>target_size: 800</code> to be a good equilibrium point between not having too little or too many points. Finally, the <code>isotope</code> refers to the beam particle, for this experiment it's \\(^{16}\\text{O}\\). </p>"},{"location":"setup/file_change_ml/#additional-ml-pipeline-parameters","title":"Additional ML Pipeline Parameters","text":"<p>The file named <code>ml_preprocessing_pipeline.py</code> puts all the different classes together the pipeline object allows us to combine certain processes together. However, as a user you have the ability to change which processes to turn \"on\" or \"off\". Note: you cannot change the order of the classes, if you would like to add one of the classes it is important to note take a look at the order they are present in <code>ml_preprocessing_steps.py</code> and add them in the same order in the pipelines.</p> <pre><code>pipeline_1 = Pipeline([\n    # (\"noise\", AttpcNoiseAddition(ratio_noise=0.1)),\n    (\"outlier\",OutlierDetection()), #getting rid of the outliers\n    (\"sampler\", UpDownScaling(target_size,isotope)),\n]) #up/down sampler \n\n# The `pipeline_2` is a data processing pipeline that consists of the following steps:\npipeline_2 = Pipeline([\n    (\"reclassify\",ReclassifyingLabels()),\n    (\"limiting\", DataLimitation(target_size)),\n    #(\"augument\", DataAugumentation(target_size=800)),\n    (\"scaling\", ScalingData()),\n]) #reclassifying and scaling (w/ concatonated dataset)\n</code></pre> <p>There are some steps of the pipeline that are required, namely:</p> <ul> <li> <p>UpDownScaling</p> </li> <li> <p>ReclassifyingLabels</p> </li> <li> <p>ScalingData</p> </li> </ul> <p>Congratulations, now you will be to run a single command to preprocess the data </p> <pre><code>python ml_preprocessing_pipeline.py\n</code></pre>"},{"location":"setup/file_change_ml/#ml-model-training-and-evaluation","title":"ML Model Training and Evaluation","text":""},{"location":"setup/file_change_ml/#configuration_1","title":"Configuration","text":"<p>We are nearing the end! I see the light at the end of the tunnel!</p> <p>The final set of parameter we have to set are the following </p> <pre><code>\"batch_options\": [128, 256],\n\"lr_options\": [3e-6,5e-6,6e-6,7.5e-6],\n\"epochs_limit\": 200,\n\"data_dir\":\"path/to/load/ml/training/data/directory/\",\n\"best_model_path\": \"path/to/save/best/model/best_model.keras\",\n\"learning_curve_path\": \"path/to/save/learning/curve/learning_curve.png\",\n\"confusion_matrix_path\": \"path/to/save/confusion/matrix/confusion_matrix.png\"\n</code></pre> <p><code>data_dir</code> is the same parameter as <code>directory_training</code> in the previous step, this is where the train, val, and test features and labels reside. <code>best_model_path</code> is where you would want your model to be saved in the form of <code>.keras</code> format, while <code>learning_curve_path</code> and <code>confusion_matrix_path</code> are the path to the learning (loss) curve and confusion matrix from model evaluation, respectively. </p> <p>We use something called early stopping for training our models, which simply means that we stop training when we don't see improvement, and because we don't have to train as long, it is easier to train multiple models with different hyperparameters to choose the best one. The <code>batch_options</code> and <code>lr_options</code> refer to batch size and learning rate options, if you would like to learn more about why these are important, check out this article. A model will be trained each combination of batch size and learning rate, choose the number of entries wisely as more entries = more combinations. </p> <p>Finally, (really finally) the <code>epoch_limit</code> parameter which serves as a default limit for how long the model will train. As mentioned above, we stop training when we don't see improvement, but what happens in the case the model keeps improving? We have to some time, and the epoch is a parameter that controls how many times the training cycle happens.</p> <p>We have reached the end, everything before actually training the model step could be done locally on a personal machine (though not advised) but training the model requires a high performance cluster, as we need GPUs, and for a considerable time. The slurm script to submit a job and request resources are different for each cluster, but this command can be used at the end to run the training python script. </p> <pre><code>python ml_training.py\n</code></pre>"},{"location":"setup/file_change_spy/","title":"Spyral","text":""},{"location":"setup/file_change_spy/#configuration","title":"Configuration","text":"<p>Since the Spyral package is a tool created to work with data from any of the AT-TPC experiments, a user has the ability to refine the experimental parameters to those of their experiment. The file we use to define these parameters is <code>run_spyral.py</code>, and these parameters have already been set for the \\(^{16}\\text{O}\\), but some user specific parameters are defined in <code>config.json</code>. </p> <p>One of the first things we will have to change will be some paths defined in</p> <pre><code>\"workspace_path\": \"/path/to/your/spyral/workspace/\",\n\"trace_path\": \"/path/to/your/spyral/traces/\",\n</code></pre> <p>Where you will be changing the <code>workspace_path</code> to a directory where all the analyzed file will go, and a <code>trace_path</code> to the directory where all the raw traces are. </p> <p>Next, we can change the runs we want to analyze and hoe many processes will be running in parallel</p> <p><pre><code>run_min = 54\nrun_max = 169\nn_processes = 17\n</code></pre> These are what the parameters need to be changed to, to analyze all the runs with 17 processes in parallel, but the suggested amount is <code>n_processes = 5</code> if being used on a personal machine.</p> <p>Finally, the last thing that could be changed by the user is the phases that are run. For the extraction of training labels and features, the list in the <code>Pipeline()</code> must be changed to </p> <p><pre><code>[true, true, true, false]\n</code></pre> This corresponds to the <code>PointcloudPhase()</code>, <code>ClusterPhase()</code>, <code>EstimationPhase()</code>, and <code>InterpSolverPhase()</code> phases in a consecutive manner. </p> <p>If you have made the changes to suit your device, you are ready to begin analyzing the data, simply run </p> <pre><code>python run_spyral\n</code></pre> <p>If you would like, you can stop here and the Spyral analysis run with the intended parameters for this experiment. However, if you would like more control over the specific methods through which each phase is run, some other parameters can be changed in <code>run_spyral.py</code> file. </p>"},{"location":"setup/file_change_spy/#additional-spyral-parameters","title":"Additional Spyral Parameters","text":"<p>Note that the argument <code>do_garfield_correction=False</code> in <code>det_params = DetectorParameters()</code> is set to <code>False</code>, this is the electric field correction and I would suggest leaving it off for this experiment but you have the ability to turn it on. </p> <p>There are two types of clustering algorithms that can be used here, currently I suggest using the one it is set to, HDBSCAN, but another clustering method called Triple Cluster can be use, though it is currently not stable. </p> <p>Even within the HDBSCAN clustering algorithm you do have the ability to change, if you would like more information about each of the arguments here, you can find (more information here).</p> <pre><code>cluster_params = ClusterParameters(\n    min_cloud_size=50,\n    hdbscan_parameters= HdbscanParameters(\n    min_points=3,min_size_scale_factor=0.05,\n    min_size_lower_cutoff=10, \n    cluster_selection_epsilon=10.0,\n    ),\n    # hdbscan_parameters= None,\n    tripclust_parameters=None,\n    # tripclust_parameters=TripclustParameters(\n    #     r=6,\n    #     rdnn=True,\n    #     k=12,\n    #     n=3,\n    #     a=0.03,\n    #     s=0.3,\n    #     sdnn=True,\n    #     t=0.0,\n    #     tauto=True,\n    #     dmax=0.0,\n    #     dmax_dnn=False,\n    #     ordered=True,\n    #     link=0,\n    #     m=50,\n    #     postprocess=False,\n    #     min_depth=25,\n    # ),\n    overlap_join=OverlapJoinParameters(\n        min_cluster_size_join=15,\n        circle_overlap_ratio=0.25,\n    ),\n    # overlap_join=None,\n    continuity_join = ContinuityJoinParameters(\n    join_radius_fraction=0.4,\n    join_z_fraction=0.2),\n    direction_threshold= 0.5,\n    outlier_scale_factor=0.1,\n\n)\n</code></pre> <p>Please note you would need to uncomment Tripclust related parameters and comment HDBSCAN ones if you would like to make that switch, and pass <code>None</code> for the one you choose not to use. </p>"},{"location":"setup/ml_preprocess_info/","title":"ML Preprocessing Steps","text":"<p>This page discusses the classes that make up the machine learning pipeline, more specifically, the custom transformers that are used within the scikit-pipeline. This is really useful because if you wanted to create another step in the pipeline, all you'd have to do is follow the same format as the ones already there. </p>"},{"location":"setup/ml_preprocess_info/#noise-addition","title":"Noise Addition","text":"<p>These two custom transformers are intended to add noise when training with simulated data, and can be turned \"off\" when training with experimental data. </p>"},{"location":"setup/ml_preprocess_info/#uniform-noise-addition","title":"Uniform Noise Addition","text":"<p>This adds random uniform noise in Cartesian coordinates by using <code>np.random.randint()</code> (more info can be found here), with the physical dimensions of the detector in mm.</p> <ul> <li>x: [-250,250]</li> <li>y: [-250,250]</li> <li>z: [0,1000] (this is the beam axis; the detector is 1 m long)</li> </ul> <p>There one input parameter for this transformer <code>ratio_noise</code>, which stands for the fraction of noise added compared to the point cloud size. </p> Figure 7: Uniform noise with two different ratio number"},{"location":"setup/ml_preprocess_info/#attpc-noise-addition","title":"ATTPC Noise Addition","text":"<p>This adds random uniform noise in z by using <code>np.random.randint()</code>, and Gaussian noise in r by <code>np.random.normal()</code> (more info can be found here).</p> <ul> <li>r: [-50,50]</li> <li>z: [0,1000] (this is the beam axis; the detector is 1 m long)</li> </ul> <p>There one input parameter for this transformer <code>ratio_noise</code>, which stands for the fraction of noise added compared to the point cloud size. </p> Figure 8: AT-TPC like noise with different ratio numbers"},{"location":"setup/ml_preprocess_info/#outlier-detection","title":"Outlier Detection","text":"<p>This transformer doesn't require any parameters but rather removes any points in the data that fall outside the physical dimensions of the detector. It is recommended to have this part of the pipeline at all times. </p>"},{"location":"setup/ml_preprocess_info/#up-down-scaling","title":"Up Down Scaling","text":"<p>This transformer has four different parameters that need to be defined by the user: <code>target_size</code>, <code>dimension</code>, and <code>isotope</code>. The isotope is the beam for the specific experiment (in this case it's <code>\"O16\"</code>); the dimension corresponds to the number of features that we will be training on, i.e. for just x,y,z (dimension = 3) but if charge is also included then it's 4. </p> <p>Finally, the <code>target_size</code> refers to the point cloud size we would be resampling to. I have chosen this to be <code>target_size = 800</code> by looking that the average size of pointcloud size for each class. </p> Figure 9: Pointcloud size for each class <p>This is another transformer that is recommended to keep in at all time.</p>"},{"location":"setup/ml_preprocess_info/#reclassifying-labels","title":"Reclassifying Labels","text":"<p>Since the ML model requires classes to begin with the label 0, we must subtract one from each label; now one track correspond to class 0, all the way to five track events corresponding to class 4. </p> <p>This transformer doesn't require a user-defined parameter. </p>"},{"location":"setup/ml_preprocess_info/#data-limitation","title":"Data Limitation","text":"<p>This transformer limits examples from all classes to the one for the lowest class, in this way the model is not biased to one class and the model evaluation metrics are a true reflection of how the model performs on all classes. </p> <p>No user-defined parameter required. </p>"},{"location":"setup/ml_preprocess_info/#data-augmentation","title":"Data Augmentation","text":"<p>In our experimental data, the one and two tracks make up majority of the data, while 3,4, and 5 track events are rare multi-track events. To \"make up\" for these numbers, we can perform something called data augmentation\u2014making multiple copies of a single event. The reason this is possible is because of azimuthal symmetry in the detector around the beam (z) axis. The parameter <code>multiplier</code> controls how many copies to create for each event.</p> <p>This transformer requires <code>target_size</code> as a parameter.</p> Figure 10: Augmentation for a class 3 event"},{"location":"setup/ml_preprocess_info/#scaling-data","title":"Scaling Data","text":"<p>Since each event has points that consists of points scattered throughout the detector, it means that every pointcloud does not span the same physical space within the detector, but we need it to. Hence, we use this transformer to scale our data with scikit <code>MinMaxScalar()</code> (more information can be found here).</p>"}]}